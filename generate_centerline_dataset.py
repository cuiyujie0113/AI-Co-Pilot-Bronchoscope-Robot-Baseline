import os
import sys
import argparse
import torch
import torchvision
import numpy as np
import cv2
import time
from PIL import Image

from lib.engine.onlineSimulation import onlineSimulationWithNetwork as onlineSimulator
from lib.utils import get_transform

np.random.seed(42)


def get_args():
    parser = argparse.ArgumentParser(description='Generate dataset from centerline trajectory',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-d', '--dataset-dir', dest='dataset_dir', type=str, default="all_groundtruth_set", 
                       help='Path of dataset directory')
    parser.add_argument('-o', '--output-dir', dest='output_dir', type=str, default="centerline_dataset", 
                       help='Output directory for generated dataset')
    parser.add_argument('--centerline-name', dest='centerline_name', type=str, 
                       default="siliconmodel3 Centerline model", help='Name of centerline to use')
    parser.add_argument('--renderer', dest='renderer', type=str, default='pyrender', 
                       choices=['pyrender', 'pybullet'], help='Renderer to use')
    parser.add_argument('--image-size', dest='image_size', type=int, default=200, 
                       help='Size of output images')
    parser.add_argument('--step-size', dest='step_size', type=int, default=1, 
                       help='Step size for sampling points along centerline')
    parser.add_argument('--max-points', dest='max_points', type=int, default=0,
                       help='If >0, limit generation to this many points (for debugging)')
    
    # æ‰¹é‡å¤„ç†å‚æ•°
    parser.add_argument('--batch', action='store_true', 
                       help='Generate datasets for multiple centerlines in batch mode')
    parser.add_argument('--start-index', dest='start_index', type=int, default=0, 
                       help='Start from specific centerline index (batch mode)')
    parser.add_argument('--end-index', dest='end_index', type=int, default=59, 
                       help='End at specific centerline index (batch mode)')
    parser.add_argument('--output-base-dir', dest='output_base_dir', type=str, default="all_centerline_datasets", 
                       help='Base output directory for batch mode')
    
    return parser.parse_args()


def generate_batch_datasets(args):
    """
    æ‰¹é‡ç”Ÿæˆå¤šæ¡ä¸­å¿ƒçº¿çš„æ•°æ®é›†
    """
    from datetime import datetime
    
    # åˆ›å»ºåŸºç¡€è¾“å‡ºç›®å½•
    if not os.path.exists(args.output_base_dir):
        os.makedirs(args.output_base_dir)
    
    # ä¸­å¿ƒçº¿åç§°åˆ—è¡¨
    centerline_names = []
    centerline_names.append("siliconmodel3 Centerline model")  # ä¸»ä¸­å¿ƒçº¿
    for i in range(1, 60):  # åˆ†æ”¯ä¸­å¿ƒçº¿ 1-59
        centerline_names.append(f"siliconmodel3 Centerline model_{i}")
    
    print(f"=== æ‰¹é‡æ•°æ®é›†ç”Ÿæˆæ¨¡å¼ ===")
    print(f"æ€»ä¸­å¿ƒçº¿æ•°é‡: {len(centerline_names)}")
    print(f"å¤„ç†èŒƒå›´: {args.start_index} åˆ° {args.end_index}")
    print(f"è¾“å‡ºåŸºç¡€ç›®å½•: {args.output_base_dir}")
    print("=" * 60)
    
    # è®°å½•å¤„ç†ç»“æœ
    success_count = 0
    failed_centerlines = []
    start_time = datetime.now()
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_file = os.path.join(args.output_base_dir, "generation_log.txt")
    with open(log_file, 'w') as f:
        f.write(f"ä¸­å¿ƒçº¿æ•°æ®é›†ç”Ÿæˆæ—¥å¿—\n")
        f.write(f"å¼€å§‹æ—¶é—´: {start_time}\n")
        f.write(f"æ€»ä¸­å¿ƒçº¿æ•°: {len(centerline_names)}\n")
        f.write(f"å¤„ç†èŒƒå›´: {args.start_index} åˆ° {args.end_index}\n")
        f.write("=" * 50 + "\n")
    
    # å¤„ç†æŒ‡å®šèŒƒå›´çš„ä¸­å¿ƒçº¿
    for idx in range(args.start_index, min(args.end_index + 1, len(centerline_names))):
        centerline_name = centerline_names[idx]
        print(f"\n[{idx+1}/{len(centerline_names)}] æ­£åœ¨å¤„ç†: {centerline_name}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•åç§°
        safe_name = centerline_name.replace(" ", "_").replace("model", "model")
        output_dir = os.path.join(args.output_base_dir, f"centerline_{idx:02d}_{safe_name}")
        
        try:
            # æ„å»ºsubprocesså‘½ä»¤ï¼ˆå•ä¸ªä¸­å¿ƒçº¿æ¨¡å¼ï¼‰
            cmd = [
                sys.executable, __file__,  # ä½¿ç”¨å½“å‰Pythonè§£é‡Šå™¨å’Œè„šæœ¬
                "--dataset-dir", args.dataset_dir,
                "--output-dir", output_dir,
                "--centerline-name", centerline_name,
                "--renderer", args.renderer,
                "--image-size", str(args.image_size),
                "--step-size", str(args.step_size)
                # æ³¨æ„ï¼šä¸ä¼  --batch å‚æ•°ï¼Œè¿™æ ·ä¼šè¿›å…¥å•ä¸ªä¸­å¿ƒçº¿æ¨¡å¼
            ]
            
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            print(f"è¾“å‡ºç›®å½•: {output_dir}")
            
            # æ‰§è¡Œsubprocess
            import subprocess
            result = subprocess.run(cmd, capture_output=True, text=True, 
                                  timeout=1800, encoding='utf-8', errors='replace')  # 30åˆ†é’Ÿè¶…æ—¶
            
            if result.returncode == 0:
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸç”Ÿæˆäº†å›¾åƒ
                rgb_dir = os.path.join(output_dir, "rgb_images")
                if os.path.exists(rgb_dir):
                    image_count = len([f for f in os.listdir(rgb_dir) if f.endswith('.jpg')])
                    print(f"âœ“ æˆåŠŸä¸º {centerline_name} ç”Ÿæˆäº† {image_count} å¯¹å›¾åƒ")
                    success_count += 1
                    
                    # è®°å½•æˆåŠŸä¿¡æ¯
                    with open(log_file, 'a', encoding='utf-8') as f:
                        f.write(f"æˆåŠŸ: {centerline_name} -> {output_dir} ({image_count} å¯¹å›¾åƒ)\n")
                else:
                    print(f"âœ— {centerline_name} - è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
                    failed_centerlines.append((centerline_name, "è¾“å‡ºç›®å½•ä¸å­˜åœ¨"))
                    
                    with open(log_file, 'a', encoding='utf-8') as f:
                        f.write(f"å¤±è´¥: {centerline_name} - è¾“å‡ºç›®å½•ä¸å­˜åœ¨\n")
            else:
                error_msg = result.stderr.strip() if result.stderr else "æœªçŸ¥é”™è¯¯"
                print(f"âœ— {centerline_name} å¤„ç†å¤±è´¥")
                print(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
                failed_centerlines.append((centerline_name, error_msg))
                
                with open(log_file, 'a', encoding='utf-8') as f:
                    f.write(f"å¤±è´¥: {centerline_name}\n")
                    f.write(f"é”™è¯¯: {error_msg}\n")
                    
        except subprocess.TimeoutExpired:
            print(f"âœ— {centerline_name} å¤„ç†è¶…æ—¶")
            failed_centerlines.append((centerline_name, "å¤„ç†è¶…æ—¶"))
            
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(f"è¶…æ—¶: {centerline_name}\n")
                
        except Exception as e:
            print(f"âœ— å¤„ç† {centerline_name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            failed_centerlines.append((centerline_name, str(e)))
            
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(f"å¼‚å¸¸: {centerline_name} - {str(e)}\n")
    
    # å®Œæˆç»Ÿè®¡
    end_time = datetime.now()
    duration = end_time - start_time
    
    print("\n" + "=" * 60)
    print("æ‰¹é‡å¤„ç†å®Œæˆ")
    print(f"æ€»è®¡å¤„ç†: {success_count + len(failed_centerlines)}")
    print(f"æˆåŠŸ: {success_count}")
    print(f"å¤±è´¥: {len(failed_centerlines)}")
    print(f"è€—æ—¶: {duration}")
    
    if failed_centerlines:
        print("\nå¤±è´¥çš„ä¸­å¿ƒçº¿:")
        for name, error in failed_centerlines:
            print(f"  - {name}: {error}")
    
    # å†™å…¥æœ€ç»ˆç»Ÿè®¡
    with open(log_file, 'a') as f:
        f.write("=" * 50 + "\n")
        f.write(f"å®Œæˆæ—¶é—´: {end_time}\n")
        f.write(f"è€—æ—¶: {duration}\n")
        f.write(f"æˆåŠŸ: {success_count}\n")
        f.write(f"å¤±è´¥: {len(failed_centerlines)}\n")
        
        if failed_centerlines:
            f.write("\nå¤±è´¥çš„ä¸­å¿ƒçº¿:\n")
            for name, error in failed_centerlines:
                f.write(f"  - {name}: {error}\n")
    
    print(f"\næ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
    return success_count, failed_centerlines


def generate_centerline_dataset(args):
    """
    ç”Ÿæˆä¸­å¿ƒçº¿è½¨è¿¹æ•°æ®é›†
    åœ¨ä¸­å¿ƒçº¿çš„æ¯ä¸ªç‚¹ä¸Šç”ŸæˆRGBå›¾åƒå’Œæ·±åº¦å›¾åƒ
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = args.output_dir
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    rgb_dir = os.path.join(output_dir, "rgb_images")
    depth_dir = os.path.join(output_dir, "depth_images")
    info_dir = os.path.join(output_dir, "camera_info")
    
    for dir_path in [rgb_dir, depth_dir, info_dir]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
    
    # åˆå§‹åŒ–ä»¿çœŸå™¨
    print(f"Initializing simulator for centerline: {args.centerline_name}")
    simulator = onlineSimulator(args.dataset_dir, args.centerline_name, 
                               renderer=args.renderer, training=False)
    
    # è·å–ä¸­å¿ƒçº¿è½¨è¿¹ç‚¹
    centerline_points = simulator.centerlineArray
    print(f"Total centerline points: {len(centerline_points)}")
    
    # åˆ›å»ºä¿¡æ¯æ–‡ä»¶
    info_file = os.path.join(info_dir, "dataset_info.txt")
    with open(info_file, 'w') as f:
        f.write("# Dataset Information\n")
        f.write(f"centerline_name: {args.centerline_name}\n")
        f.write(f"renderer: {args.renderer}\n")
        f.write(f"image_size: {args.image_size}\n")
        f.write(f"total_points: {len(centerline_points)}\n")
        f.write(f"step_size: {args.step_size}\n")
        f.write("# Format: index, x, y, z, pitch, yaw, distance_to_start\n")
    
    # éå†ä¸­å¿ƒçº¿ç‚¹ç”Ÿæˆæ•°æ®
    total_distance = 0.0
    generated_count = 0
    
    for i in range(0, len(centerline_points), args.step_size):
        try:
            point = centerline_points[i]
            print(f"Processing point {i}/{len(centerline_points)}: {point}")
            
            # è®¡ç®—ç›¸æœºå§¿æ€ï¼ˆåŸºäºä¸­å¿ƒçº¿æ–¹å‘ï¼‰
            if i < len(centerline_points) - 1:
                # ä½¿ç”¨ä¸‹ä¸€ä¸ªç‚¹è®¡ç®—æ–¹å‘
                direction_vector = centerline_points[i + 1] - point
            elif i > 0:
                # æœ€åä¸€ä¸ªç‚¹ï¼Œä½¿ç”¨å‰ä¸€ä¸ªç‚¹è®¡ç®—æ–¹å‘
                direction_vector = point - centerline_points[i - 1]
            else:
                # åªæœ‰ä¸€ä¸ªç‚¹çš„æƒ…å†µ
                direction_vector = np.array([0, 1, 0])
            
            # ç”Ÿæˆå›¾åƒï¼ˆä½¿ç”¨æ­£ç¡®çš„ dir æ–¹æ³•ï¼‰
            rgb_img, depth_img = generate_images_at_point_with_direction(
                simulator, point, direction_vector, args.image_size
            )
            
            # è®¡ç®—pitchå’Œyawç”¨äºè®°å½•ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
            direction_norm = np.linalg.norm(direction_vector)
            if direction_norm > 1e-6:
                normalized_direction = direction_vector / direction_norm
                pitch = np.arcsin(np.clip(normalized_direction[2], -1.0, 1.0))
                if abs(normalized_direction[0]) > 1e-6 or abs(normalized_direction[1]) > 1e-6:
                    yaw = np.arctan2(normalized_direction[0], normalized_direction[1])
                else:
                    yaw = 0.0
                pitch_deg = pitch * 180.0 / np.pi
                yaw_deg = yaw * 180.0 / np.pi
            else:
                pitch_deg = yaw_deg = 0.0
            
            # ä¿å­˜å›¾åƒ
            rgb_filename = f"rgb_{i:06d}.jpg"
            depth_filename = f"depth_{i:06d}.jpg"
            
            cv2.imwrite(os.path.join(rgb_dir, rgb_filename), rgb_img)
            cv2.imwrite(os.path.join(depth_dir, depth_filename), depth_img)
            
            # è®¡ç®—åˆ°èµ·å§‹ç‚¹çš„è·ç¦»
            if i > 0:
                distance_diff = np.linalg.norm(point - centerline_points[i - args.step_size])
                total_distance += distance_diff
            
            # ä¿å­˜ç›¸æœºä¿¡æ¯
            with open(info_file, 'a') as f:
                f.write(f"{i}, {point[0]:.6f}, {point[1]:.6f}, {point[2]:.6f}, "
                       f"{pitch_deg:.3f}, {yaw_deg:.3f}, {total_distance:.6f}\n")
            
            generated_count += 1
            # å¦‚æœè®¾ç½®äº† max_pointsï¼Œåˆ™é™åˆ¶ç”Ÿæˆæ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if args.max_points > 0 and generated_count >= args.max_points:
                print(f"Reached max_points={args.max_points}, stopping early")
                break
            
            if generated_count % 10 == 0:
                print(f"Generated {generated_count} image pairs")
                
        except Exception as e:
            print(f"Error processing point {i}: {e}")
            continue
    
    print(f"Dataset generation completed!")
    print(f"Total images generated: {generated_count}")
    print(f"RGB images saved to: {rgb_dir}")
    print(f"Depth images saved to: {depth_dir}")
    print(f"Camera info saved to: {info_file}")
    
    return generated_count


def generate_images_at_point_with_direction(simulator, position, direction_vector, image_size):
    """
    åœ¨æŒ‡å®šä½ç½®ä½¿ç”¨æ­£ç¡®çš„æ–¹å‘å‘é‡ç”ŸæˆRGBå’Œæ·±åº¦å›¾åƒï¼ˆä½¿ç”¨WORKING diræ–¹æ³•ï¼‰
    """
    # ä½¿ç”¨æ­£ç¡®çš„ 'dir' æ–¹æ³•æ„å»ºç›¸æœºå§¿æ€
    # forward = normalized dir_vec, up = [0,0,1] (if nearly parallel choose alternative)
    f = direction_vector / (np.linalg.norm(direction_vector) + 1e-12)
    up = np.array([0.0, 0.0, 1.0])
    if abs(np.dot(f, up)) > 0.999:
        up = np.array([0.0, 1.0, 0.0])
    r = np.cross(up, f)
    r = r / (np.linalg.norm(r) + 1e-12)
    u = np.cross(f, r)
    R = np.vstack([r, u, f]).T
    pose = np.eye(4)
    pose[:3, :3] = R
    pose[:3, 3] = position
    
    # è®¾ç½®å…‰ç…§å¼ºåº¦
    light_intensity = 0.3
    
    # æ¸…ç©ºåœºæ™¯å¹¶æ·»åŠ èŠ‚ç‚¹
    simulator.scene.clear()
    simulator.scene.add_node(simulator.fuze_node)
    
    # æ·»åŠ å…‰æºå’Œç›¸æœº
    from pyrender import SpotLight
    spot_l = SpotLight(color=np.ones(3), intensity=light_intensity,
                      innerConeAngle=0, outerConeAngle=np.pi/2, range=1)
    spot_l_node = simulator.scene.add(spot_l, pose=pose)
    cam_node = simulator.scene.add(simulator.cam, pose=pose)
    
    # è®¾ç½®å§¿æ€
    simulator.scene.set_pose(spot_l_node, pose)
    simulator.scene.set_pose(cam_node, pose)
    
    # æ¸²æŸ“å›¾åƒ
    rgb_img, depth_img = simulator.r.render(simulator.scene)
    rgb_img = rgb_img[:, :, :3]
    
    # è‡ªåŠ¨è°ƒæ•´å…‰ç…§å¼ºåº¦ä»¥è·å¾—åˆé€‚çš„äº®åº¦
    mean_intensity = np.mean(rgb_img)
    target_intensity = 140
    tolerance = 20
    max_iterations = 50
    iteration = 0
    
    min_light = 0.001
    max_light = 20.0
    
    while abs(mean_intensity - target_intensity) > tolerance and iteration < max_iterations:
        if mean_intensity > target_intensity:
            max_light = light_intensity
        else:
            min_light = light_intensity
        
        light_intensity = (min_light + max_light) / 2
        
        # é‡æ–°æ¸²æŸ“
        simulator.scene.clear()
        simulator.scene.add_node(simulator.fuze_node)
        
        spot_l = SpotLight(color=np.ones(3), intensity=light_intensity,
                          innerConeAngle=0, outerConeAngle=np.pi/2, range=1)
        spot_l_node = simulator.scene.add(spot_l, pose=pose)
        cam_node = simulator.scene.add(simulator.cam, pose=pose)
        
        simulator.scene.set_pose(spot_l_node, pose)
        simulator.scene.set_pose(cam_node, pose)
        
        rgb_img, depth_img = simulator.r.render(simulator.scene)
        rgb_img = rgb_img[:, :, :3]
        mean_intensity = np.mean(rgb_img)
        
        iteration += 1
    
    # å¤„ç†æ·±åº¦å›¾
    depth_img[depth_img == 0] = 0.5
    depth_img[depth_img > 0.5] = 0.5
    depth_img = depth_img / 0.5 * 255
    depth_img = depth_img.astype(np.uint8)
    
    # è°ƒæ•´å›¾åƒå¤§å°
    rgb_img = cv2.resize(rgb_img, (image_size, image_size))
    depth_img = cv2.resize(depth_img, (image_size, image_size))
    
    # è½¬æ¢RGBåˆ°BGRï¼ˆOpenCVæ ¼å¼ï¼‰
    rgb_img = rgb_img[:, :, ::-1]
    
    return rgb_img, depth_img


def generate_images_at_point(simulator, position, pitch, yaw, image_size):
    """
    åœ¨æŒ‡å®šä½ç½®å’Œå§¿æ€ç”ŸæˆRGBå’Œæ·±åº¦å›¾åƒ
    """
    # è½¬æ¢è§’åº¦ä¸ºå¼§åº¦
    # pitch, yaw are provided in degrees; convert to radians
    pitch_rad = pitch / 180.0 * np.pi
    yaw_rad = yaw / 180.0 * np.pi
    
    # åˆ›å»ºç›¸æœºå§¿æ€çŸ©é˜µ
    import pybullet as p
    # Match onlineSimulation.run convention: add +pi/2 to pitch and use Euler order [pitch, 0, yaw]
    # (onlineSimulation applies pitch = pitch/180*pi + pi/2 before building quaternion)
    pitch_rad = pitch_rad + np.pi / 2.0
    quat = p.getQuaternionFromEuler([pitch_rad, 0.0, yaw_rad])
    R = p.getMatrixFromQuaternion(quat)
    R = np.reshape(R, (3, 3))
    
    pose = np.identity(4)
    pose[:3, 3] = position
    pose[:3, :3] = R
    
    # è®¾ç½®å…‰ç…§å¼ºåº¦
    light_intensity = 0.3
    
    # æ¸…ç©ºåœºæ™¯å¹¶æ·»åŠ èŠ‚ç‚¹
    simulator.scene.clear()
    simulator.scene.add_node(simulator.fuze_node)
    
    # æ·»åŠ å…‰æºå’Œç›¸æœº
    from pyrender import SpotLight
    spot_l = SpotLight(color=np.ones(3), intensity=light_intensity,
                      innerConeAngle=0, outerConeAngle=np.pi/2, range=1)
    spot_l_node = simulator.scene.add(spot_l, pose=pose)
    cam_node = simulator.scene.add(simulator.cam, pose=pose)
    
    # è®¾ç½®å§¿æ€
    simulator.scene.set_pose(spot_l_node, pose)
    simulator.scene.set_pose(cam_node, pose)
    
    # æ¸²æŸ“å›¾åƒ
    rgb_img, depth_img = simulator.r.render(simulator.scene)
    rgb_img = rgb_img[:, :, :3]
    
    # è‡ªåŠ¨è°ƒæ•´å…‰ç…§å¼ºåº¦ä»¥è·å¾—åˆé€‚çš„äº®åº¦
    mean_intensity = np.mean(rgb_img)
    target_intensity = 140
    tolerance = 20
    max_iterations = 50
    iteration = 0
    
    min_light = 0.001
    max_light = 20.0
    
    while abs(mean_intensity - target_intensity) > tolerance and iteration < max_iterations:
        if mean_intensity > target_intensity:
            max_light = light_intensity
        else:
            min_light = light_intensity
        
        light_intensity = (min_light + max_light) / 2
        
        # é‡æ–°æ¸²æŸ“
        simulator.scene.clear()
        simulator.scene.add_node(simulator.fuze_node)
        
        spot_l = SpotLight(color=np.ones(3), intensity=light_intensity,
                          innerConeAngle=0, outerConeAngle=np.pi/2, range=1)
        spot_l_node = simulator.scene.add(spot_l, pose=pose)
        cam_node = simulator.scene.add(simulator.cam, pose=pose)
        
        simulator.scene.set_pose(spot_l_node, pose)
        simulator.scene.set_pose(cam_node, pose)
        
        rgb_img, depth_img = simulator.r.render(simulator.scene)
        rgb_img = rgb_img[:, :, :3]
        mean_intensity = np.mean(rgb_img)
        
        iteration += 1
    
    # å¤„ç†æ·±åº¦å›¾åƒ
    depth_img[depth_img == 0] = 0.5
    depth_img[depth_img > 0.5] = 0.5
    depth_img = depth_img / 0.5 * 255
    depth_img = depth_img.astype(np.uint8)
    
    # è°ƒæ•´å›¾åƒå°ºå¯¸
    rgb_img = cv2.resize(rgb_img, (image_size, image_size))
    depth_img = cv2.resize(depth_img, (image_size, image_size))
    
    # è½¬æ¢RGBé¢œè‰²é€šé“é¡ºåº (RGB -> BGR for OpenCV)
    rgb_img = rgb_img[:, :, ::-1]
    
    return rgb_img, depth_img


def main():
    args = get_args()
    
    if args.batch:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        print("=== ä¸­å¿ƒçº¿æ•°æ®é›†æ‰¹é‡ç”Ÿæˆå™¨ ===")
        print(f"æ¸²æŸ“å™¨: {args.renderer}")
        print(f"å›¾åƒå°ºå¯¸: {args.image_size}x{args.image_size}")
        print(f"æ­¥é•¿: {args.step_size}")
        print(f"å¤„ç†èŒƒå›´: {args.start_index} åˆ° {args.end_index}")
        print("=" * 50)
        
        success_count, failed_centerlines = generate_batch_datasets(args)
        
        if failed_centerlines:
            print(f"\næ³¨æ„ï¼šæœ‰ {len(failed_centerlines)} æ¡ä¸­å¿ƒçº¿å¤„ç†å¤±è´¥")
            print("æ‚¨å¯ä»¥ä½¿ç”¨ --start-index å’Œ --end-index å‚æ•°é‡æ–°å¤„ç†å¤±è´¥çš„éƒ¨åˆ†")
        else:
            print(f"\nğŸ‰ æ‰€æœ‰ {success_count} æ¡ä¸­å¿ƒçº¿æ•°æ®é›†ç”ŸæˆæˆåŠŸï¼")
    else:
        # å•ä¸ªä¸­å¿ƒçº¿æ¨¡å¼
        print("=== ä¸­å¿ƒçº¿æ•°æ®é›†ç”Ÿæˆå™¨ ===")
        print(f"ä¸­å¿ƒçº¿: {args.centerline_name}")
        print(f"æ¸²æŸ“å™¨: {args.renderer}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"å›¾åƒå°ºå¯¸: {args.image_size}x{args.image_size}")
        print(f"æ­¥é•¿: {args.step_size}")
        print("=" * 40)
        
        # ç”Ÿæˆæ•°æ®é›†
        generated_count = generate_centerline_dataset(args)
        
        print("=" * 40)
        print(f"æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼ç”Ÿæˆäº† {generated_count} å¯¹å›¾åƒã€‚")


if __name__ == '__main__':
    main()
